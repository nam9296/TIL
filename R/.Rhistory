v1<-c(20,10,50,40,30)
v2<-c(300,200,100,700,600)
v3<-c("a","b","b","a","b")
c3
df<-data.frame(v1,v2,v3)
df
v1
sort(v1)
sort(v1,decreasing = TRUE)
v1
order(v1)
v1[order(v1)]
df
order(df,v1)
order(df)
v1
order(v1)
order(v2)
order(v3)
order(-v2)
v1[order(-v1)]
order(v2)
df[order(v1),]
df[order(v1,-v2,v3),]
v1<-c(20,10,50,40,20)
v2<-c(300,200,100,700,600)
v3<-c("a","b","b","a","b")
df<-data.frame(v1,v2,v3)
df
df[order(v1,-v2,v3),]
install.packages("plyr")
library(plyr)
arrange(df, v1)
arrange(df, v1,v2)
arrange(df, v1,-v2)
arrange(df, v1,desc(v2),v3)
sort(letters,decreasing = TRUE)
x<-c(2,-1,3,7,0,5,8)
x%%2
x%%2==0
x[x%%2==0]
x[x%%2!=0]
which(x%%2!=0)
x[which(x%%2!=0)]
x[-which(x%%2!=0)]
iris
ex <- c(4.0, 3.0, 1.5, 0.15)
temp <- t(iris[1:4])
iris$distance <- sqrt(colSums((temp-ex)^2))
table(head(iris[order(iris$distance),],9)$Species)
max(table(head(iris[order(iris$distance),],9)$Species)
)
max(table(head(iris[order(iris$distance),],9)$Species))
iris
iris
data(iris)
iris
mean(iris$Sepal.Length)
mean(iris$Sepal.Width)
mean(iris$Petal.Length)
mean(iris$Sepal.Width)
sd(iris$Sepal.Length)
sd(iris$Sepal.Width)
sd(iris$Petal.Length)
sd(iris$Sepal.Width)
misl<-mean(iris$Sepal.Length)
misw<-mean(iris$Sepal.Width)
mipl<-mean(iris$Petal.Length)
mipw<-mean(iris$Sepal.Width)
misl<-mean(iris$Sepal.Length)
misw<-mean(iris$Sepal.Width)
mipl<-mean(iris$Petal.Length)
mipw<-mean(iris$Sepal.Width)
misl<-sd(iris$Sepal.Length)
misw<-sd(iris$Sepal.Width)
mipl<-sd(iris$Petal.Length)
mipw<-sd(iris$Sepal.Width)
misl<-mean(iris$Sepal.Length)
misw<-mean(iris$Sepal.Width)
mipl<-mean(iris$Petal.Length)
mipw<-mean(iris$Sepal.Width)
sisl<-sd(iris$Sepal.Length)
sisw<-sd(iris$Sepal.Width)
sipl<-sd(iris$Petal.Length)
sipw<-sd(iris$Sepal.Width)
misl
dim(t(iris[1:4]))
tempIris<-t(iris[1:4])
tempIris
tempIris-c(misl,misw,mipl,mipw)
(tempIris-c(misl,misw,mipl,mipw))/c(sisl,sisw,sipl,sipw)
siris<-(tempIris-c(misl,misw,mipl,mipw))/c(sisl,sisw,sipl,sipw)
siris
iris(rbind(iris,ex))
tail(iris)
# 6. iris데이터에서
# Sepal.Length Sepal.Width Petal.Length Petal.Width 값이 각각
# (4.0, 3.0, 1.5, 0.15)일때 예상되는 종(setosa, verisicolor, virginica)을 출력하시오
# (4.0, 3.0, 1.5, 0.15)와 가장 유클리디안 거리가 가까운 데이터를 9개 찾아낸다
# 9개의 데이터에 대한 종별 빈도수를 기반으로 유추
# ex) setosa:6, veri:2, vir:1 => setosa 예상
iris
ex <- c(4.0, 3.0, 1.5, 0.15)
iris(rbind(iris,ex))
tail(iris)
iris<-(rbind(iris,ex))
tail(iris)
temp <- t(iris[1:4])
t(siris)
iris
scale(iris[,1])
data(iris)
scale(iris[,1])
data(iris)
(iris[,1]-mean(iris[,1]))
(iris[,1]-mean(iris[,1]))/
sd(iris[,1])
(iris[,1]-mean(iris[,1]))/sd(iris[,1])
head(iris[,1]-mean(iris[,1]))/sd(iris[,1])
head(scale(iris[,1]))
class(head(iris[,1]-mean(iris[,1]))/sd(iris[,1]))
class(head(scale(iris[,1])) )
# 정규화
iris[,1]-min(iris[,1])
# 정규화
(iris[,1]-min(iris[,1]))/ (max(iris[,1])- min(iris[,1]))
scale(iris[,1])
cbind(iris, scale=scale(iris[,-5]))
# apply(데이터, 행(1)/열(2), 함수)
apply(iris[,-5],2,scale)
apply(iris[,-5],1,scale)
apply(iris[,-5],2,function(x){(x-mean(x))/sd(x)})
apply(iris[,-5],2,function(x){(x-mean(x,na.rm=TRUE))/sd(x,na.rm=TRUE)})
# 데이터에 na가 있다면
siris<-apply(iris[,-5],2,function(x){(x-mean(x,na.rm=TRUE))/sd(x,na.rm=TRUE)})
cbind(iris,siris)
cbind(iris,ss=scale(iris,[,-5]))
cbind(iris,ss=scale(iris[,-5]))
scale(iris[,-5])
apply(iris[,-5],2,mean)
apply(iris[,-5],2,sd)
apply(iris[,-5],2,var)
apply(iris[,-5],2,mean)
x<- "We have a dream"
nchar(x)
length(x)
length(c("we","have","a", "dream"))
y<-c("we","have","a", "dream")
y[4]
length(y[4])
char(y[4])
nchar(y[4])
tolower(x)
toupper(x)
x
split(x,'')
strsplit(x,'')
# 문장 쪼개기
strsplit(x,split=" ")
strsplit(x,split="")
class(strsplit(x,split=" "))
# strsplit 결과가 리스트로 저장
strsplit(x,split=" ")
# 리스트를 벡터로 바꿔볼까?
unlist(strsplit(x,split=" ") )
class(unlist(strsplit(x,split=" ") )
)
strsplit(x,split=" ")[[1]]
res<-strsplit(x,split=" ")[[1]]
res[4]
x1<- "We have a dream"
x2<- "How are you"
x3<- "I am fine"
myword<-c(x1,x2,x3)
myword
length(myword)
strsplit(myword," ")
strsplit(myword," ")[[3]]
strsplit(myword," ")[[3]][3]
said<- "WHAT IS ESSENTIAL is invisible to the Eye"
strsplit(said," ")
said.word<-strsplit(said," ")
# unique() : 유일한 단어를 추출
unique(said.word)
unique(said.word[[1]])
unique(tolower(said.word[[1]]))
# paste 함수에 sep, collapse 옵션
# Batman-wants-to-fly;captain America-wants to fly; Hulk-wants to-fly
heros<- c("Batman","Catpain America", "Hulk")
paste(hero,"wants","to","fly",sep='-')
paste(heros,"wants","to","fly",sep='-')
# outer함수
# : 두 집합에 대해 가능한 모든 순서쌍의 곱을 수행(카티전 곱; default)
outer(c(1,2,3),c(3,2,1))
asia.countries<-c("Korea","China","India")
info<-c("GDP","Population","Area")
outer(asia.countries,info, FUN=paste, sep='-')
class(outer(asia.countries,info, FUN=paste, sep='-'))
# as vector() : 행렬 -> 벡터형식
as.vector(out)
out<-outer(asia.countries,info, FUN=paste, sep='-')
# as vector() : 행렬 -> 벡터형식
as.vector(out)
outer(asia.countries, FUN=paste, sep='-')
outer(asia.countries,asia.countries FUN=paste, sep='-')
outer(asia.countries,asia.countries, FUN=paste, sep='-')
res
res<-outer(asia.countries,asia.countries, FUN=paste, sep='-')
res
lower.tri(res)
res[lower.tri(res)]
# 상삼각행렬
upper.tri(res)
res[upper.tri(res)]
!lower.tri(res)
res[!lower.tri(res)]
# substr: 텍스트에 특정 부분 문자열을 추출
substr("Data Analyics")
# substr: 텍스트에 특정 부분 문자열을 추출
substr("Data Analyics",1:4)
# substr: 텍스트에 특정 부분 문자열을 추출
substr("Data Analyics",1,4)
substr("Data Analyics",7,14)
substr("Data Analyics",6,14)
# substring() : 텍스트의 특정 부분 문자열 추출, 끝위치 생량
substring("Data Analyics",6)
c("Data Analyics","Data Mining", "Data Visualization")
myclass<-c("Data Analyics","Data Mining", "Data Visualization")
substr(myclass,1,4)
substr(myclass,nchar(myclass)-5,4)
substr(myclass,nchar(myclass)-5,nchar(myclass))
island
islands
class(islands)
names(islands)
landmasses<-names(islands)
landmasses<-names(islands)
landmasses
# New 문자열이 포함된 단어의 인덱스를 추출
grep(pattern="New", x=landmasses)
# grep():
# New 문자열이 포함된 단어의 인덱스를 추출
index<-grep(pattern="New", x=landmasses)
grep[index]
landmasses[index]
grep(pattern = "New", landmasses, value=TRUE)
grep(pattern = " ", landmasses, value=TRUE)
landmasses[grep(pattern = " ", landmasses)]
txt<- " Data Anal is useful. Data Anal is interesting."
sub(pattern = "Anal", replacement = Analyist)
sub(pattern = "Anal", replacement = Analyist,txt)
sub(pattern = "Anal", replacement = "Analyist",txt)
sub(pattern = "Data", replacement = "Business",txt)
gsub(pattern = "Anal", replacement = "Analyist",txt)
x<-c("input.csv","data.csv","big.csv")
gsub(pattern=".csv",replacement = " ",x)
gsub(pattern=".csv",replacement = "",x)
df<-read.csv("Samsung.csv",header = TRUE, sep=",")
df<-read.csv("Samsung.csv",header = TRUE, sep=",")
df<-read.csv("Samsung.csv", sep=",")
df<-read.csv("samsung.csv", sep=",")
df<-read.csv("samsung.csv", header =TRUE, sep=",")
df<-read.csv("samsung.csv", header =TRUE, sep=",")
df
df2<-read.csv("samsung.csv")
df2
class(df)
df
df<-read.csv("samsung.csv", header =TRUE, sep=",")
df
df
x<-c("Happy","Birthday","to","you")
length(x)
nchar(x)
sum(nchar(x))
x<- "Happy Birthday to you"
length(x)
nchar(x)
y<-c("A 1" "B 2" "C 3" "D 4" "E 5")
x<-c("Happy","Birthday","to","you")
paste(x,collapse = " ")
length(x)
nchar(x)
sum(nchar(x))
x<-paste(x,collapse = " ")
length(x)
sum(nchar(x))
nchar(x)
gsub(pattern=" ",replacement = "",y)
y<-c("A 1", "B 2", "C 3", "D 4", "E 5")
gsub(pattern=" ",replacement = "",y)
# 4. 단어 단위로 분할하고, 모든 쉼표와 하이픈은 제거하시오.
# c("Yesterday is history,", "That's, why we call it the present - from Kung")
z<-c("Yesterday is history,", "That's, why we call it the present - from Kung")
z
gsub(pattern=" ",replacement = "",z)
strsplit(z,split=" ")
gsub(pattern=",","-",replacement = "",z)
gsub(pattern=",",replacement = "",z)
gsub(pattern="-",replacement = "",z)
z<-gsub(pattern=",",replacement = "",z)
z<-gsub(pattern="-",replacement = "",z)
z
z<-strsplit(z,split=" ")
z
c("110101-1234123", "950102-2121212")
k<-c("110101-1234123", "950102-2121212")
jumin<-c("110101-1234123", "950102-2121212")
jumin
substring(Jumin,6)
jumin<-c("110101-1234123", "950102-2121212")
substring(Jumin,6)
substr(Jumin,6,10)
substr(jumin,6,10)
substring(jumin,10)
substring(jumin,8)
substring(jumin,8)<-"*******"
jumin
month.abb
paste(month.abb,c("_1","_2","_3","_4","_5","_6","_7","_8","_9","_10","_11","_12"))
mon<-paste(month.abb,c("_1","_2","_3","_4","_5","_6","_7","_8","_9","_10","_11","_12"))
strsplit(mon,split="-")
mon<-paste(month.abb,c("_1","_2","_3","_4","_5","_6","_7","_8","_9","_10","_11","_12"))
mon
df<-read.csv("samsung.csv")
# 1) Open, High, Low, Close, Adj.Close, Volume 열을 추출
row.names(df)
# 1) Open, High, Low, Close, Adj.Close, Volume 열을 추출
rnames(df)
# 1) Open, High, Low, Close, Adj.Close, Volume 열을 추출
colnames(df)
df[1]
df[-1]
min(df[2:]
min(df[2:])
min(df[2:5])
min(df[2])
min(df[2])
min(df[3])
min(df[4])
min(df[5])
min(df[7])
min(df[6])
max(df[2])
max(df[3])
max(df[4])
max(df[5])
max(df[6])
max(df[7])
# apply(데이터, 행(1)/열(2), 함수)
apply(iris[,-5],2,scale) # 열단위로 한것
ex <- c(4.0, 3.0, 1.5, 0.15)
iris<-(rbind(iris,ex))
tail(iris)
temp <- t(iris[1:4])
iris$distance <- sqrt(colSums((temp-ex)^2))
table(head(iris[order(iris$distance),],9)$Species)
# 2) 각 열에 대해 최소값, 최대값, 평균, 표준편차 출력
class(df)
mean(df[-1])
mean(df)
apply(df, 1, mean)
df<-read.csv("samsung.csv")
# 1) Open, High, Low, Close, Adj.Close, Volume 열을 추출
colnames(df)
df[-1]
# 2) 각 열에 대해 최소값, 최대값, 평균, 표준편차 출력
class(df)
apply(df, 1, mean)
# 2) 각 열에 대해 최소값, 최대값, 평균, 표준편차 출력
class(df)
df
paste(month.abb,1:12,sep='_',collapse='-')
df[-1]
df[,-1]
df.min <- apply(df[,-1],2,min)
df.max <- apply(df[,-1],2,max)
df.mean <- apply(df[,-1],2,mean)
df.sd <- apply(df[,-1],2,sd)
df.min
df.max
df.mean
df.sd
paste(month.abb,1:12,sep='_',collapse='-')
apply(df[-1],2,sd,na.rm=T)
apply(df[-1],2,min,na.rm=T)
apply(df[-1],2,max,na.rm=T)
apply(df[-1],2,mean,na.rm=T)
apply(df[-1],2,sd,na.rm=T)
apply(df[,-1],2, min, na.rm=TRUE)
apply(df[,-1],2, max, na.rm=TRUE)
apply(df[,-1],2, mean ,na.rm=TRUE)
apply(df[,-1],2, sd, na.rm=TRUE)
# 3) 각 열에 대해 정규화 / 표준화
apply(df[-1],2,scale)
apply(df[-1],2,function(x){(x-min(x,na.rm=T))/(max(x,na.rm=T)-min(x,na.rm=T))})
# 3) 각 열에 대해 정규화 / 표준화
apply(df[-1],2,scale)
# 3) 각 열에 대해 정규화 / 표준화
apply(df[,-1],2,function(x){(x-mean(x))/sd(x)})
apply(df[,-1],2,function(x){(x-min(x,na.rm=T))/(max(x,na.rm=T)-min(x,na.rm=T))})
# 3) 각 열에 대해 정규화 / 표준화
# 정규화
apply(df[,-1],2,function(x){(x-min(x))/(max(x)-min(x))})
# 표준화
apply(df[,-1],2,function(x){(x-mean(x))/sd(x)})
df<-read.csv("samsung.csv")
# 1) Open, High, Low, Close, Adj.Close, Volume 열을 추출
colnames(df)
df[,-1]
# 2) 각 열에 대해 최소값, 최대값, 평균, 표준편차 출력
apply(df[,-1],2, min, na.rm=TRUE)
apply(df[,-1],2, max, na.rm=TRUE)
apply(df[,-1],2, mean ,na.rm=TRUE)
apply(df[,-1],2, sd, na.rm=TRUE)
# 3) 각 열에 대해 정규화 / 표준화
# 정규화
apply(df[,-1],2,function(x){(x-min(x))/(max(x)-min(x))})
# 표준화
apply(df[,-1],2,function(x){(x-mean(x))/sd(x)})
# 3) 각 열에 대해 정규화 / 표준화
# 정규화
apply(df[,-1],2,function(x){(x-min(x))/(max(x)-min(x))})
apply(df[-1],2,scale)
apply(df[-1],2,function(x){(x-min(x,na.rm=T))/(max(x,na.rm=T)-min(x,na.rm=T))})
# 3) 각 열에 대해 정규화 / 표준화
# 정규화
apply(df[,-1],2,function(x){(x-min(x))/(max(x)-min(x))})
apply(df[-1],2,scale)
apply(df[-1],2,function(x){(x-min(x,na.rm=T))/(max(x,na.rm=T)-min(x,na.rm=T))})
# 표준화
apply(df[,-1],2,function(x){(x-mean(x))/sd(x)})
apply(df[-1],2,scale)
apply(df[,-1],2, sd)
apply(df[,-1],2, min)
apply(df[,-1],2, max)
apply(df[,-1],2, mean)
apply(df[,-1],2, sd)
apply(df[,3:4], 2, function(x){max(x[1:nrow(df)-1]-x[2:nrow(df)])})
nrow(df)
High<- abs(df[,2][1:248]-df[,2][2:249])
High
max(high)
max(High)
Low<- abs(df[,2][1:248]-df[,2][2:249])
min(Low)
Low<- abs(df[,3][1:248]-df[,3][2:249])
min(Low)
max(Low)
x<-paste(x,collapse = " ")
length(x)
sum(nchar(x))
x
# 표준화
apply(df[,-1],2,function(x){(x-mean(x))/sd(x)})
download.file(url=url,destfile = "myIris.csv")
#https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
url<-"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
iris.uci<-read.csv(url,header = FALSE)
iris.uci
download.file(url=url,destfile = "myIris.csv")
#http://seanlahman.com/files/database/baseballdatabank-master_2016-03-02.zip
url<-"http://seanlahman.com/files/database/baseballdatabank-master_2016-03-02.zip"
local.copy<-"baseball.zip"
download.file(url,local.copy)
unzip(zipfile = local.copy, "baseballdatablank-master/core/Salaries.csv")
unzip(zipfile = local.copy, "baseballdatabank-master/core/Salaries.csv")
#https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
url<-"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
iris.uci<-read.csv(url,header = FALSE)
iris.uci
download.file(url=url,destfile = "myIris.csv")
#http://seanlahman.com/files/database/baseballdatabank-master_2016-03-02.zip
url<-"http://seanlahman.com/files/database/baseballdatabank-master_2016-03-02.zip"
local.copy<-"baseball.zip"
download.file(url,local.copy)
bs<-read.csv(unzip(zipfile = local.copy, "baseballdatabank-master/core/Salaries.csv"))
bs
install.packages("ggplot2")
library("ggplot")
library("ggplot2")
mpg
mpg$manufacturer
install.packages("dplyr")
install.packages("dplyr")
library(dplyr)
#rename(데이터프레임,변경후컬럼이름=변경전 컬럼이름)
midwest
#rename(데이터프레임,변경후컬럼이름=변경전 컬럼이름)
midwest
midwest<-as.data.frame(midwest) # tibble-> dataframe
library("ggplot2")
mpg
mpg$manufacturer
midwest<-as.data.frame(midwest) # tibble-> dataframe
library(dplyr)
#rename(데이터프레임,변경후컬럼이름=변경전 컬럼이름)
midwest
rename(midwest, cg=category)
